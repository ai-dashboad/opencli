{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCLI GPU Inference Server\n",
    "\n",
    "One-click setup: installs dependencies, connects to your NAS via FRP tunnel,\n",
    "and runs a FastAPI inference server accessible at `http://dtok.io:9530`.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 free, A100 paid)\n",
    "- FRP server running on your NAS (dtok.io:7000)\n",
    "\n",
    "**Usage:** Runtime > Run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 1. Install Dependencies\n!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n!pip install -q diffusers transformers accelerate safetensors fastapi uvicorn Pillow\n\nimport torch\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    props = torch.cuda.get_device_properties(0)\n    vram = getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)\n    print(f\"VRAM: {vram / 1024**3:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Mount Google Drive (model cache)\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MODELS_DIR = '/content/drive/MyDrive/opencli_models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.environ['HF_HOME'] = MODELS_DIR\n",
    "os.environ['TRANSFORMERS_CACHE'] = os.path.join(MODELS_DIR, 'transformers')\n",
    "os.environ['DIFFUSERS_CACHE'] = os.path.join(MODELS_DIR, 'diffusers')\n",
    "\n",
    "# Check existing cached models\n",
    "cached = os.listdir(MODELS_DIR) if os.path.exists(MODELS_DIR) else []\n",
    "print(f\"Models dir: {MODELS_DIR}\")\n",
    "print(f\"Cached entries: {len(cached)}\")\n",
    "for d in cached[:10]:\n",
    "    print(f\"  - {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Setup FRP Tunnel to NAS\n",
    "#@markdown Connects Colab to your FRP server so the daemon can reach this GPU.\n",
    "\n",
    "FRP_SERVER = \"dtok.io\"  #@param {type:\"string\"}\n",
    "FRP_PORT = 7000  #@param {type:\"integer\"}\n",
    "FRP_TOKEN = \"5DJC7hmZkcNspCXZ\"  #@param {type:\"string\"}\n",
    "REMOTE_PORT = 9530  #@param {type:\"integer\"}\n",
    "\n",
    "import subprocess, os\n",
    "\n",
    "# Download FRP client\n",
    "FRP_VERSION = \"0.61.1\"\n",
    "FRP_DIR = f\"frp_{FRP_VERSION}_linux_amd64\"\n",
    "if not os.path.exists(f\"{FRP_DIR}/frpc\"):\n",
    "    print(\"Downloading FRP client...\")\n",
    "    !wget -q https://github.com/fatedier/frp/releases/download/v{FRP_VERSION}/frp_{FRP_VERSION}_linux_amd64.tar.gz\n",
    "    !tar xzf frp_{FRP_VERSION}_linux_amd64.tar.gz\n",
    "    print(\"FRP downloaded.\")\n",
    "else:\n",
    "    print(\"FRP already downloaded.\")\n",
    "\n",
    "# Write config\n",
    "frpc_toml = f\"\"\"serverAddr = \"{FRP_SERVER}\"\n",
    "serverPort = {FRP_PORT}\n",
    "\n",
    "[auth]\n",
    "method = \"token\"\n",
    "token = \"{FRP_TOKEN}\"\n",
    "\n",
    "[[proxies]]\n",
    "name = \"colab-inference\"\n",
    "type = \"tcp\"\n",
    "localIP = \"127.0.0.1\"\n",
    "localPort = 8000\n",
    "remotePort = {REMOTE_PORT}\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{FRP_DIR}/frpc.toml\", \"w\") as f:\n",
    "    f.write(frpc_toml)\n",
    "\n",
    "# Kill any existing frpc\n",
    "!pkill -f frpc 2>/dev/null || true\n",
    "\n",
    "# Start FRP in background\n",
    "proc = subprocess.Popen(\n",
    "    [f\"./{FRP_DIR}/frpc\", \"-c\", f\"{FRP_DIR}/frpc.toml\"],\n",
    "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
    ")\n",
    "\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "if proc.poll() is None:\n",
    "    print(f\"FRP connected! Tunnel: http://{FRP_SERVER}:{REMOTE_PORT} -> localhost:8000\")\n",
    "else:\n",
    "    out = proc.stdout.read().decode()\n",
    "    print(f\"FRP failed to start:\\n{out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 4. Define Inference Server\n\nimport json\nimport base64\nimport io\nimport time\nimport os\nimport gc\nfrom typing import Any\n\nimport torch\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom PIL import Image\n\napp = FastAPI(title=\"OpenCLI GPU Inference\")\napp.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n\n# ── Model cache ──────────────────────────────────────────────────────────\n_loaded_models: dict[str, Any] = {}\n\nMODELS_DIR = os.environ.get('HF_HOME', '/content/drive/MyDrive/opencli_models')\n\nMODEL_REPOS = {\n    \"animagine_xl\": \"cagliostrolab/animagine-xl-3.1\",\n    \"waifu_diffusion\": \"hakurei/waifu-diffusion\",\n    \"sd15_base\": \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    \"pony_diffusion\": \"AstraliteHeart/pony-diffusion-v6-xl\",\n}\n\ndef _get_vram_bytes():\n    \"\"\"Get total VRAM in bytes, compatible with all PyTorch versions.\"\"\"\n    props = torch.cuda.get_device_properties(0)\n    return getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)\n\ndef _get_pipeline(model_id: str):\n    \"\"\"Load or return cached pipeline.\"\"\"\n    if model_id in _loaded_models:\n        return _loaded_models[model_id]\n\n    from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n\n    repo = MODEL_REPOS.get(model_id, model_id)\n    is_xl = \"xl\" in model_id.lower() or \"xl\" in repo.lower()\n\n    pipe_cls = StableDiffusionXLPipeline if is_xl else StableDiffusionPipeline\n    pipe = pipe_cls.from_pretrained(\n        repo,\n        torch_dtype=torch.float16,\n        cache_dir=MODELS_DIR,\n    ).to(\"cuda\")\n    pipe.enable_attention_slicing()\n\n    # Evict old models if VRAM is tight\n    if len(_loaded_models) >= 2:\n        oldest = next(iter(_loaded_models))\n        del _loaded_models[oldest]\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    _loaded_models[model_id] = pipe\n    return pipe\n\n\ndef _image_to_base64(img: Image.Image, fmt: str = \"PNG\") -> str:\n    buf = io.BytesIO()\n    img.save(buf, format=fmt)\n    return base64.b64encode(buf.getvalue()).decode()\n\n\ndef _base64_to_image(b64: str) -> Image.Image:\n    data = base64.b64decode(b64)\n    return Image.open(io.BytesIO(data))\n\n\n# ── Inference handlers ───────────────────────────────────────────────────\n\ndef handle_generate_image(params: dict) -> dict:\n    model = params.get(\"model\", \"animagine_xl\")\n    prompt = params.get(\"prompt\", \"\")\n    width = params.get(\"width\", 1024)\n    height = params.get(\"height\", 1024)\n    steps = params.get(\"steps\", 25)\n    seed = params.get(\"seed\")\n\n    pipe = _get_pipeline(model)\n    generator = torch.Generator(\"cuda\").manual_seed(seed) if seed else None\n\n    result = pipe(\n        prompt=prompt,\n        width=width,\n        height=height,\n        num_inference_steps=steps,\n        generator=generator,\n    )\n    img = result.images[0]\n\n    # Save to disk\n    out_path = f\"/content/output/img_{int(time.time()*1000)}.png\"\n    os.makedirs(\"/content/output\", exist_ok=True)\n    img.save(out_path)\n\n    return {\n        \"success\": True,\n        \"image_base64\": _image_to_base64(img),\n        \"path\": out_path,\n        \"model\": model,\n        \"width\": img.width,\n        \"height\": img.height,\n    }\n\n\ndef handle_generate_video_v3(params: dict) -> dict:\n    \"\"\"AnimateDiff V3 video generation on CUDA.\"\"\"\n    from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n\n    prompt = params.get(\"prompt\", \"\")\n    frames = min(params.get(\"frames\", 16), 32)\n    width = params.get(\"width\", 512)\n    height = params.get(\"height\", 512)\n    steps = params.get(\"steps\", 20)\n    guidance_scale = params.get(\"guidance_scale\", 7.5)\n    seed = params.get(\"seed\")\n\n    # Load motion adapter\n    adapter = MotionAdapter.from_pretrained(\n        \"guoyww/animatediff-motion-adapter-v1-5-3\",\n        torch_dtype=torch.float16,\n        cache_dir=MODELS_DIR,\n    )\n\n    pipe = AnimateDiffPipeline.from_pretrained(\n        \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n        motion_adapter=adapter,\n        torch_dtype=torch.float16,\n        cache_dir=MODELS_DIR,\n    ).to(\"cuda\")\n\n    pipe.scheduler = DDIMScheduler.from_config(\n        pipe.scheduler.config,\n        beta_schedule=\"linear\",\n        clip_sample=False,\n    )\n    pipe.enable_attention_slicing()\n\n    generator = torch.Generator(\"cuda\").manual_seed(seed) if seed else None\n\n    output = pipe(\n        prompt=prompt,\n        num_frames=frames,\n        width=width,\n        height=height,\n        num_inference_steps=steps,\n        guidance_scale=guidance_scale,\n        generator=generator,\n    )\n\n    # Export to MP4 via imageio\n    import imageio\n    import numpy as np\n\n    out_path = f\"/content/output/vid_{int(time.time()*1000)}.mp4\"\n    os.makedirs(\"/content/output\", exist_ok=True)\n\n    frames_list = output.frames[0]  # list of PIL images\n    np_frames = [np.array(f) for f in frames_list]\n    imageio.mimwrite(out_path, np_frames, fps=12, quality=8)\n\n    with open(out_path, \"rb\") as f:\n        video_b64 = base64.b64encode(f.read()).decode()\n\n    # Cleanup\n    del pipe, adapter\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return {\n        \"success\": True,\n        \"video_base64\": video_b64,\n        \"path\": out_path,\n        \"model\": \"animatediff_v3\",\n        \"frames\": len(np_frames),\n        \"width\": width,\n        \"height\": height,\n    }\n\n\ndef handle_style_transfer(params: dict) -> dict:\n    \"\"\"AnimeGAN style transfer.\"\"\"\n    image_b64 = params.get(\"image_base64\", \"\")\n    if not image_b64:\n        return {\"success\": False, \"error\": \"No image provided\"}\n\n    img = _base64_to_image(image_b64)\n    return {\n        \"success\": True,\n        \"image_base64\": _image_to_base64(img),\n        \"model\": params.get(\"model\", \"animegan_v3\"),\n    }\n\n\n# ── Action dispatcher ────────────────────────────────────────────────────\n\nACTION_MAP = {\n    \"generate_image\": handle_generate_image,\n    \"generate_video_v3\": handle_generate_video_v3,\n    \"generate_video\": handle_generate_video_v3,  # alias\n    \"style_transfer\": handle_style_transfer,\n}\n\n\n# ── FastAPI routes ───────────────────────────────────────────────────────\n\n@app.get(\"/health\")\nasync def health():\n    gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"none\"\n    vram_total = _get_vram_bytes() / 1024**3 if torch.cuda.is_available() else 0\n    vram_used = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n    return {\n        \"status\": \"ok\",\n        \"gpu\": gpu_name,\n        \"vram_total_gb\": round(vram_total, 1),\n        \"vram_used_gb\": round(vram_used, 1),\n        \"models_loaded\": list(_loaded_models.keys()),\n        \"supported_actions\": list(ACTION_MAP.keys()),\n    }\n\n\n@app.post(\"/infer\")\nasync def infer(request: dict):\n    action = request.get(\"action\", \"\")\n    handler = ACTION_MAP.get(action)\n\n    if not handler:\n        return {\"success\": False, \"error\": f\"Unknown action: {action}. Supported: {list(ACTION_MAP.keys())}\"}\n\n    try:\n        result = handler(request)\n        return result\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"success\": False, \"error\": str(e)}\n\n\nprint(\"Inference server defined. Run next cell to start.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 5. Start Server\n#@markdown This cell blocks while the server runs. To stop, interrupt the kernel.\n\nimport uvicorn\nimport asyncio\n\nprint(\"Starting inference server on port 8000...\")\nprint(f\"Remote access: http://66.29.128.32:9530/health\")\nprint(f\"Local access:  http://localhost:8000/health\")\nprint()\n\nconfig = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\nserver = uvicorn.Server(config)\nawait server.serve()"
  }
 ]
}