import 'dart:io';
import 'dart:convert';
import 'dart:async';
import '../core/plugin.dart';

/// è¯­éŸ³è¯†åˆ«æ’ä»¶ - ä½¿ç”¨ Whisper æˆ– macOS åŸç”Ÿ API
class SpeechRecognitionPlugin extends Plugin {
  @override
  String get name => 'speech_recognition';

  @override
  String get version => '1.0.0';

  @override
  String get description => 'Speech to text using Whisper or native APIs';

  String _whisperModel = 'base'; // tiny, base, small, medium, large
  bool _useWhisper = true;

  @override
  Future<void> initialize() async {
    print('ğŸ¤ Initializing Speech Recognition Plugin...');

    // æ£€æŸ¥ Whisper æ˜¯å¦å¯ç”¨
    try {
      final result = await Process.run('which', ['whisper']);
      if (result.exitCode == 0) {
        print('âœ“ Whisper found: ${result.stdout.toString().trim()}');
        _useWhisper = true;
      } else {
        print('âš ï¸  Whisper not found, will use macOS native API');
        _useWhisper = false;
      }
    } catch (e) {
      print('âš ï¸  Could not check for Whisper: $e');
      _useWhisper = false;
    }

    print('âœ“ Speech Recognition Plugin initialized');
  }

  @override
  Future<Map<String, dynamic>> handleTask(
    String taskType,
    Map<String, dynamic> taskData,
  ) async {
    if (taskType == 'speech_to_text') {
      return await _transcribeAudio(taskData);
    }

    throw UnimplementedError('Task type $taskType not supported');
  }

  /// è½¬æ¢éŸ³é¢‘ä¸ºæ–‡å­—
  Future<Map<String, dynamic>> _transcribeAudio(
    Map<String, dynamic> data,
  ) async {
    final audioData = data['audio'] as String?; // base64 encoded
    final audioPath = data['audio_path'] as String?;
    final language = data['language'] as String? ?? 'Chinese';

    String tempAudioFile;

    if (audioPath != null) {
      tempAudioFile = audioPath;
    } else if (audioData != null) {
      // ä¿å­˜ base64 éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
      tempAudioFile = await _saveAudioData(audioData);
    } else {
      throw ArgumentError('Either audio or audio_path must be provided');
    }

    try {
      String transcription;

      if (_useWhisper) {
        transcription = await _transcribeWithWhisper(tempAudioFile, language);
      } else {
        transcription = await _transcribeWithMacOS(tempAudioFile);
      }

      return {
        'success': true,
        'text': transcription,
        'method': _useWhisper ? 'whisper' : 'macos_native',
        'language': language,
      };
    } catch (e) {
      return {
        'success': false,
        'error': e.toString(),
      };
    } finally {
      // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      if (audioPath == null && audioData != null) {
        await File(tempAudioFile).delete();
      }
    }
  }

  /// ä½¿ç”¨ Whisper è½¬å½•
  Future<String> _transcribeWithWhisper(
    String audioPath,
    String language,
  ) async {
    print('ğŸ¤ Transcribing with Whisper (model: $_whisperModel)...');

    final result = await Process.run('whisper', [
      audioPath,
      '--model',
      _whisperModel,
      '--language',
      language,
      '--output_format',
      'txt',
      '--output_dir',
      '/tmp',
    ]);

    if (result.exitCode != 0) {
      throw Exception('Whisper failed: ${result.stderr}');
    }

    // è¯»å–è¾“å‡ºæ–‡ä»¶
    final audioFileName = audioPath.split('/').last.split('.').first;
    final outputFile = File('/tmp/$audioFileName.txt');

    if (await outputFile.exists()) {
      final text = await outputFile.readAsString();
      await outputFile.delete();
      return text.trim();
    }

    throw Exception('Whisper output file not found');
  }

  /// ä½¿ç”¨ macOS åŸç”Ÿ API è½¬å½•
  Future<String> _transcribeWithMacOS(String audioPath) async {
    print('ğŸ¤ Transcribing with macOS native API...');

    // ä½¿ç”¨ AppleScript è°ƒç”¨ macOS è¯­éŸ³è¯†åˆ«
    final script = '''
on run argv
    set audioFile to item 1 of argv
    tell application "System Events"
        -- macOS doesn't have direct command-line speech recognition
        -- This is a placeholder for native implementation
        return "macOS native recognition not implemented yet"
    end tell
end run
''';

    final tempScript = await File('/tmp/speech_recognition.scpt').create();
    await tempScript.writeAsString(script);

    final result = await Process.run('osascript', [
      tempScript.path,
      audioPath,
    ]);

    await tempScript.delete();

    if (result.exitCode != 0) {
      throw Exception('macOS recognition failed: ${result.stderr}');
    }

    return result.stdout.toString().trim();
  }

  /// ä¿å­˜ base64 éŸ³é¢‘æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶
  Future<String> _saveAudioData(String base64Audio) async {
    final bytes = base64Decode(base64Audio);
    final tempFile =
        File('/tmp/audio_${DateTime.now().millisecondsSinceEpoch}.m4a');
    await tempFile.writeAsBytes(bytes);
    return tempFile.path;
  }

  @override
  Future<void> dispose() async {
    print('ğŸ¤ Speech Recognition Plugin disposed');
  }

  @override
  List<String> get supportedTaskTypes => ['speech_to_text'];
}
